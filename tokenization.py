# -*- coding: utf-8 -*-
"""tokenization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yBKtOYro9FxquyhbIyysZ2GJkxR_1Wm5
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

sentences = [
    'I love my dog',
    'I love my cat',
    'Do you think my dog is amazing no'
]

tokenizer = Tokenizer(num_words = 100, oov_token = '<OOV>')
tokenizer.fit_on_texts(sentences)
word_index = tokenizer.word_index
print("\nword index: {}".format(word_index))

sequences = tokenizer.texts_to_sequences(sentences)
print("\n sentences: {}".format(sequences))

padded = pad_sequences(sequences, padding = 'post')
print("\n padded= {}".format(padded))

test_sentence = [
    'i really love my dog',
    'my dog loves my manatee',
    'my dog loves my manatee my dog loves my manatee',
    'my dog loves my manatee my dog loves my manatee yes or no'
]

test_sequence = tokenizer.texts_to_sequences(test_sentence)
print("\n test sequence = {}".format(test_sequence))

test_padding = pad_sequences(test_sequence, padding = 'post', maxlen = 10, truncating = 'post')
print("\n test_padding = {}".format(test_padding))